{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"test.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"toc-showmarkdowntxt":false},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"gA67svR0gFi6"},"source":["# Non-stationary texture synthesis using adversarial expansions"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"colab_type":"code","executionInfo":{"elapsed":2643,"status":"ok","timestamp":1572346717172,"user":{"displayName":"Optie's Animation Works","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBj3sMVkhsxItPC_Im2TsABg_eIaxTmbYgvkWzYgA=s64","userId":"04911501212954216143"},"user_tz":-540},"id":"lG5U2IO2IUaW","outputId":"9d720d97-eb67-4dbe-bd75-26c9eba8b402"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/gdrive\n"]}],"source":["import os\n","from google.colab import drive\n","\n","DIR = \"/content/gdrive/My Drive/sandbox/Chainer_NonStationaryTextureSynthesis/\"\n","IMG_NAMES = [\"spiral.jpg\", \"003.jpg\"]\n","\n","drive.mount('/content/gdrive')\n","os.chdir(DIR)"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"5hIpNrCJmnAp"},"outputs":[],"source":["#!pip install chainer-6.4.0.tar.gz\n","#!pip install 'cupy-cuda100>=6.4.0,<7.0.0'"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":252},"colab_type":"code","executionInfo":{"elapsed":3016,"status":"ok","timestamp":1572346717565,"user":{"displayName":"Optie's Animation Works","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBj3sMVkhsxItPC_Im2TsABg_eIaxTmbYgvkWzYgA=s64","userId":"04911501212954216143"},"user_tz":-540},"id":"1PTyNunQgFi7","outputId":"57febd3d-3ef7-4791-c147-73b3bec22396"},"outputs":[{"name":"stdout","output_type":"stream","text":["Platform: Linux-4.14.137+-x86_64-with-Ubuntu-18.04-bionic\n","Chainer: 5.4.0\n","NumPy: 1.17.3\n","CuPy:\n","  CuPy Version          : 5.4.0\n","  CUDA Root             : /usr/local/cuda\n","  CUDA Build Version    : 10000\n","  CUDA Driver Version   : 10010\n","  CUDA Runtime Version  : 10000\n","  cuDNN Build Version   : 7301\n","  cuDNN Version         : 7301\n","  NCCL Build Version    : 2402\n","  NCCL Runtime Version  : 2402\n","iDeep: 2.0.0.post3\n"]}],"source":["from PIL import Image\n","\n","import numpy as np\n","from numpy.lib.stride_tricks import as_strided\n","import matplotlib\n","import cv2\n","matplotlib.use('Agg')\n","\n","import chainer\n","from chainer import cuda\n","import chainer.functions as F\n","import chainer.links as L\n","from chainer import Variable\n","from chainer.training import extensions\n","\n","chainer.print_runtime_info()"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"vsriX-RygFjA"},"source":["## Residual Block"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"LbZzsrr5gFjB"},"outputs":[],"source":["class ResBlock(chainer.Chain):\n","    def __init__(self, ch, bn=True, activation=F.relu):\n","        self.use_bn = bn\n","        self.activation = activation\n","        super(ResBlock, self).__init__()\n","        with self.init_scope():\n","            self.c1 = L.Convolution2D(ch, ch, 3, 1, 1)\n","            self.c2 = L.Convolution2D(ch, ch, 3, 1, 1)\n","            self.bn1 = L.BatchNormalization(ch)\n","            self.bn2 = L.BatchNormalization(ch)\n","\n","    def forward(self, x):\n","        h = self.c1(x)\n","        h = self.bn1(h) if self.use_bn else h\n","        h = self.activation(h)\n","        h = self.c2(h)\n","        h = self.bn2(h) if self.use_bn else h\n","        return h + x"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"drAAHYVvgFjD"},"source":["## CBR (Conv + Normalization + Activation)"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"9jqLhkTWgFjE"},"outputs":[],"source":["# discriminator の訓練時に乗せるノイズ\n","def add_noise(h, sigma=0.2):\n","    if not chainer.config.train:\n","        return h\n","    xp = cuda.get_array_module(h.data)\n","    return h + sigma * xp.random.randn(*h.data.shape)\n","\n","# (ksize, strides, padding)\n","ksp = {\n","    'down': (4, 2, 1),\n","    'none-9': (9, 1, 4),\n","    'none-7': (7, 1, 3),\n","    'none-5': (5, 1, 2),\n","    'none': (3, 1, 1),\n","    'up': (3, 1, 1),\n","    'none-4': (4, 1, 1),\n","    'none-1': (1, 1, 0)\n","}\n","\n","class CBR(chainer.Chain):\n","    def __init__(self, ch0, ch1, bn=True, sample='down', activation=F.relu, dropout=False, noise=False):\n","        self.use_bn = bn\n","        self.activation = activation\n","        self.dropout = dropout\n","        self.sample = sample\n","        self.noise = noise\n","        w = chainer.initializers.Normal(0.02)\n","        super(CBR, self).__init__()\n","        with self.init_scope():\n","            self.c = L.Convolution2D(ch0, ch1, *(ksp[sample]), initialW=w)\n","            if self.use_bn:\n","                self.bn = L.BatchNormalization(ch1)\n","\n","    def forward(self, x):\n","        #h = L.Deconvolution2D(3, 1, 1)(x) if self.sample == \"up\" else self.c(x)\n","        h = F.unpooling_2d(x, 2, 2, 0, cover_all=False) if self.sample == \"up\" else x\n","        h = self.c(h)\n","\n","        h = self.bn(h) if self.use_bn else h\n","        h = add_noise(h) if self.noise else h\n","        h = F.dropout(h) if self.dropout else h\n","        h = self.activation(h) if self.activation else h\n","        return h"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"E6DHKLvUgFjH"},"source":["## Generator"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"iqOAKPDQgFjI"},"outputs":[],"source":["class Generator(chainer.ChainList):\n","    def __init__(self):\n","        super(Generator, self).__init__()\n","        with self.init_scope():\n","            layers = [\n","                CBR(3, 3, bn=True, sample='none-7'),\n","                CBR(3, 64, bn=True, sample='none'),\n","                CBR(64, 128, bn=True, sample='down'),\n","                CBR(128, 256, bn=True, sample='down'),\n","                ResBlock(256, bn=True),\n","                ResBlock(256, bn=True),\n","                ResBlock(256, bn=True),\n","                ResBlock(256, bn=True),\n","                ResBlock(256, bn=True),\n","                ResBlock(256, bn=True),\n","                CBR(256, 512, bn=True, sample='none'),\n","                CBR(512, 256, bn=True, sample='up'),\n","                CBR(256, 128, bn=True, sample='up'),\n","                CBR(128, 64, bn=True, sample='up'),\n","                CBR(64, 3, bn=True, sample='none-7', activation=F.tanh)\n","            ]\n","            for layer in layers:\n","                self.add_link(layer)\n","\n","    def __call__(self, x):\n","        for f in self.children():\n","            x = f(x)\n","        return x"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"z8-JNsbCgFjK"},"source":["## Discriminator"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"26JZLkmLgFjL"},"outputs":[],"source":["class Discriminator(chainer.ChainList):\n","    def __init__(self, in_ch=3, n_down_layers=4):\n","        super(Discriminator, self).__init__()\n","        with self.init_scope():\n","            layers = [\n","                CBR(3, 64, bn=False, sample='down', activation=F.leaky_relu, noise=True),\n","                CBR(64, 128, bn=True, sample='down', activation=F.leaky_relu, noise=True),\n","                CBR(128, 256, bn=True, sample='down', activation=F.leaky_relu, noise=True),\n","                CBR(256, 512, bn=True, sample='down', activation=F.leaky_relu, noise=True),\n","                CBR(512, 512, bn=True, sample='none-4', activation=F.leaky_relu, noise=True),\n","                CBR(512, 1, bn=False, sample='none-1', activation=None, noise=True)\n","            ]\n","            \n","            for layer in layers:\n","                self.add_link(layer)\n","\n","\n","    def __call__(self, x):\n","        x = add_noise(x)\n","        for f in self.children():\n","            x = f(x)\n","        return x"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"2nCx-xS7gFjN"},"source":["## Updater"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"JKf1M3nKgFjO"},"outputs":[],"source":["def make_optimizer(model, alpha=0.0002, beta1=0.5):\n","    optimizer = chainer.optimizers.Adam(alpha=alpha, beta1=beta1)\n","    optimizer.setup(model)\n","    optimizer.add_hook(chainer.optimizer_hooks.WeightDecay(0.0001), 'hook_dec')\n","    return optimizer"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"JW5v76xhgFjY"},"outputs":[],"source":["gen = Generator().to_gpu()\n","dis = Discriminator().to_gpu()\n","\n","opt_gen = make_optimizer(gen)\n","opt_dis = make_optimizer(dis)"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"OdYzO1RugFjb"},"outputs":[],"source":["style_layers = ['conv1_1', 'conv2_1', 'conv3_1', 'conv4_1', 'conv5_1']\n","# 1000/(64 x 64), 1000/(128 x 128), 1000/(256 x 256), 1000/(512 x 512), 1000/(512 x 512)\n","style_weights = [0.244, 0.061, 0.015, 0.004, 0.004]\n","\n","class DCGANUpdater(chainer.training.updaters.StandardUpdater):\n","\n","    def __init__(self, *args, **kwargs):\n","        self.gen, self.dis = kwargs.pop('models')\n","        self.vgg19 = L.VGG19Layers()\n","        self.vgg19.to_gpu()\n","        super(DCGANUpdater, self).__init__(*args, **kwargs)\n","\n","    def loss_dis(self, dis, y_fake, y_real):\n","        xp = dis.xp\n","        batchsize = len(y_fake)\n","        #L1 = F.mean_squared_error(y_real, xp.ones(y_real.array.shape, dtype='float32'))\n","        #L2 = F.mean_squared_error(y_fake, xp.zeros(y_fake.array.shape, dtype='float32'))\n","        L1 = F.sum(F.softplus(-y_real)) / batchsize\n","        L2 = F.sum(F.softplus(y_fake)) / batchsize\n","        loss = L1 / 2. + L2 / 2.\n","        chainer.report({'loss': loss}, dis)\n","        chainer.report({'loss/false_fake': L1}, dis)\n","        chainer.report({'loss/false_real': L2}, dis)\n","        return loss\n","    \n","    def gram_matrix(self, y):\n","        xp = self.gen.xp\n","        b, ch, h, w = y.data.shape\n","        features = F.reshape(y, (b, ch, w*h))\n","        gram = F.matmul(features, features, transb=True) / xp.float32(2*ch*w*h)**2\n","        return gram\n","\n","    def loss_gen(self, gen, y_fake, x_real, x_fake):\n","        xp = gen.xp\n","        \n","        with chainer.using_config('train', False):\n","            with chainer.using_config('enable_backprop', False):\n","                x_real_feat = self.vgg19.extract([(chainer.backends.cuda.to_cpu(x_real.array)[0][:, :, ::-1] + 1) *127.5], layers=style_layers)\n","                x_fake_feat = self.vgg19.extract([(chainer.backends.cuda.to_cpu(x_fake.array)[0][:, :, ::-1] + 1) *127.5], layers=style_layers)\n","        \n","        loss_style = 0\n","        for layer, w in zip(style_layers, style_weights):\n","            loss_style += w * F.mean_squared_error(\n","                self.gram_matrix(F.relu(x_real_feat[layer])),\n","                self.gram_matrix(F.relu(x_fake_feat[layer])),\n","            )\n","        \n","        batchsize = len(y_fake)\n","        loss_L1 = F.mean_absolute_error(x_real, x_fake)\n","        #loss_adv = F.mean_squared_error(y_fake, xp.ones(y_fake.array.shape, dtype='float32')) / 2.\n","        loss_adv = F.sum(F.softplus(-y_fake)) / batchsize\n","        \n","        alpha = 100\n","        beta = 1\n","\n","\n","        loss = loss_adv + alpha * loss_L1 + beta * loss_style\n","        \n","        chainer.report({'loss': loss}, gen)\n","        chainer.report({'loss/adv': loss_adv}, gen)\n","        chainer.report({'loss/L1': alpha * loss_L1}, gen)\n","        chainer.report({'loss/style': beta * loss_style}, gen)\n","        return loss\n","\n","    def update_core(self):\n","        gen_optimizer = self.get_optimizer('gen')\n","        dis_optimizer = self.get_optimizer('dis')\n","        batch = self.get_iterator('main').__next__()\n","        batchsize = len(batch)\n","\n","        xp = self.gen.xp\n","        ch, rk, ck = batch[0][0].shape\n","        ch, r2k, c2k = batch[0][1].shape\n","\n","        batch_k = xp.zeros((batchsize, ch, rk, ck)).astype(\"f\")\n","        batch_2k = xp.zeros((batchsize, ch, r2k, c2k)).astype(\"f\")\n","\n","        for i in range(batchsize):\n","            batch_k[i, :] = xp.asarray(batch[i][0])\n","            batch_2k[i, :] = xp.asarray(batch[i][1])\n","\n","        gen, dis = self.gen, self.dis\n","\n","        # x_real: 入力画像からの2kパッチ\n","        x_real = Variable(batch_2k)\n","        y_real = dis(x_real)\n","\n","        xp = chainer.backends.cuda.get_array_module(x_real.data)\n","\n","        # x_fake: kパッチから生成した2kパッチ\n","        x_fake = gen(batch_k)\n","        y_fake = dis(x_fake)\n","\n","        dis_optimizer.update(self.loss_dis, dis, y_fake, y_real)\n","        gen_optimizer.update(self.loss_gen, gen, y_fake, x_real, x_fake)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"gjUgtBDPgFjd"},"source":["## Dataset"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"rWrmGKZogFje"},"outputs":[],"source":["class PatchDataset(chainer.dataset.DatasetMixin):\n","    def __init__(self, image_path, crop_size=128):\n","        self.image = cv2.imread(image_path).astype('float32')\n","        #self.image = cv2.pyrDown(cv2.pyrDown(self.image))\n","        #cv2.imwrite(image_path+'small.jpg', self.image.astype('uint8'))\n","        self.k = crop_size\n","        assert isinstance(self.k, int) and 1<=self.k and 2*self.k<=min(*self.image.shape[:2]) \n","        r, c, ch = self.image.shape\n","        self.n_patch_of_k = (r-self.k) * (c-self.k)\n","        self.n_patch_of_2k = (r-2*self.k) * (c-2*self.k)\n","    \n","    def __len__(self):\n","        return self.n_patch_of_k\n","    \n","    def get_example(self, i):\n","        r, c, ch = self.image.shape\n","        \n","        # 2k*2k patch の左上のindex\n","        r0_2k = np.random.randint(0, r - 2*self.k)\n","        c0_2k = np.random.randint(0, c - 2*self.k)\n","        # k*k patch をその中から取る\n","        r0_k = np.random.randint(r0_2k, r0_2k + self.k - 1)\n","        c0_k = np.random.randint(c0_2k, c0_2k + self.k - 1)\n","        \n","        img_k = self.image[r0_k:(r0_k + self.k), c0_k:(c0_k + self.k), :]\n","        img_k = self.preprocess(img_k)\n","        img_2k = self.image[r0_2k:(r0_2k + 2*self.k), c0_2k:(c0_2k + 2*self.k), :]\n","        img_2k = self.preprocess(img_2k)\n","        \n","        return img_k, img_2k\n","    \n","    def preprocess(self, img):\n","        img = img.astype(\"f\")\n","        img = img / 127.5 - 1\n","        img = img.transpose((2, 0, 1))\n","        return img\n","\n","    def postprocess(self, img):\n","        img = (img + 1) *127.5\n","        img = np.clip(img, 0, 255)\n","        img = img.astype(np.uint8)\n","        img = img.transpose((1, 2, 0))\n","        return img\n","        \n","        "]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"u0m4ED9kgFjh"},"source":["## Let's train"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"QGPN2mARfH2J"},"source":["### params"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"FoA4q361gFji"},"outputs":[],"source":["n_iter = 100000  # number of epochs\n","batchsize = 1  # minibatch size\n","snapshot_interval = 1000  # number of iterations per snapshots\n","imsave_interval = 1000\n","display_interval = 100  # number of iterations per display the status\n","plot_interval = 100\n","gpu_id = 0\n","seed = 0  # random seed\n","\n","in_dir = 'dataset/'\n","tmp_train_snapshot = 'last_snapshot.npz'\n","\n","def out_generated_image(gen, dataset, seed, dst_path):\n","    @chainer.training.make_extension()\n","    def make_image(trainer):\n","        np.random.seed(seed)\n","        xp = gen.xp\n","        img = xp.asarray(dataset.preprocess(dataset.image))\n","        with chainer.using_config('train', False):\n","            x = gen(img.reshape((1,*img.shape)))\n","        x = chainer.backends.cuda.to_cpu(x.array)\n","        x = dataset.postprocess(x[0])\n","\n","        preview_path = dst_path + 'output_itr{:0>8}.jpg'.format(trainer.updater.iteration)\n","        if not os.path.exists(dst_path):\n","            os.makedirs(dst_path)\n","        cv2.imwrite(preview_path, x)\n","    return make_image"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"3Oe9GGX9gFjs"},"source":["### extensions & run"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"349n5s8SgFjt"},"outputs":[],"source":["%%capture\n","for imname in IMG_NAMES:\n","    in_img = in_dir + imname\n","    out_dir = 'out/' + imname + '_result/'\n","    out_img_dir = out_dir + 'img/'\n","\n","    patchDataset = PatchDataset(in_img)\n","\n","    train_iter = chainer.iterators.SerialIterator(patchDataset, batchsize)\n","\n","    updater = DCGANUpdater(models=(gen, dis), iterator=train_iter,\n","                        optimizer={ 'gen': opt_gen, 'dis': opt_dis }, device=gpu_id)\n","    trainer = chainer.training.Trainer(updater, (n_iter, 'iteration'), out=out_dir)\n","\n","    trainer.extend(\n","        extensions.snapshot(filename=tmp_train_snapshot),\n","        trigger=(snapshot_interval, 'iteration')\n","    )\n","    trainer.extend(\n","        extensions.snapshot_object(gen, 'gen.npz'),\n","        trigger=(snapshot_interval, 'iteration')\n","    )\n","    trainer.extend(\n","        extensions.snapshot_object(dis, 'dis.npz'),\n","        trigger=(snapshot_interval, 'iteration')\n","    )\n","    trainer.extend(\n","        extensions.LogReport(\n","            trigger=(display_interval, 'iteration'),\n","            log_name='log.json'\n","        )\n","    )\n","\n","    # on Colab, stdout has better be disabled for auto refresh\n","    # trainer.extend(\n","    #     extensions.PrintReport(['iteration', 'gen/loss', 'dis/loss']),\n","    #     trigger=(display_interval, 'iteration')\n","    # )\n","    # trainer.extend(extensions.ProgressBar(update_interval=plot_interval))\n","\n","    trainer.extend(\n","        extensions.PlotReport(\n","            ['gen/loss/adv', 'dis/loss'],\n","            x_key='iteration', trigger=(plot_interval, 'iteration'),\n","            file_name='adv_loss_plot.png'\n","        )\n","    )\n","\n","    trainer.extend(\n","        extensions.PlotReport(\n","            ['gen/loss/adv', 'gen/loss/L1', 'gen/loss/style'],\n","            x_key='iteration', trigger=(plot_interval, 'iteration'),\n","            file_name='gen_loss_plot.png'\n","        )\n","    )\n","\n","    trainer.extend(\n","        extensions.PlotReport(\n","            ['dis/loss/false_fake', 'dis/loss/false_real'],\n","            x_key='iteration', trigger=(plot_interval, 'iteration'),\n","            file_name='dis_loss_plot.png'\n","        )\n","    )\n","\n","    trainer.extend(\n","        out_generated_image(gen, patchDataset, seed, out_img_dir),\n","        trigger=(imsave_interval, 'iteration')\n","    )\n","\n","    if os.path.exists(out_dir+tmp_train_snapshot):\n","        chainer.serializers.load_npz(out_dir+tmp_train_snapshot, trainer)\n","\n","    trainer.run()"]}]}